# ğŸ§  Kafka + Python Event-Driven Trade Streaming System

## ğŸš€ Overview

This project simulates a **real-time trade streaming platform** using **Kafka** and **Python's async features (`aiokafka`)**. It demonstrates how to build a **modular**, **observable**, and **fault-tolerant** system for delivering trade events to risk engines or back-office processors.

---

## ğŸ“ Folder Structure

```
kafka-trade-streaming/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py              # Env-based Kafka + DLQ settings
â”‚   â”œâ”€â”€ models.py              # TradeEvent dataclass
â”‚   â”œâ”€â”€ producer.py            # Async Kafka producer with retry/backoff
â”‚   â”œâ”€â”€ consumer.py            # Async Kafka consumer with batch + DLQ
â”‚   â”œâ”€â”€ services.py            # Business logic (validation + PnL)
â”‚   â””â”€â”€ utils.py               # JSON logger setup
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_services.py       # Pytest unit tests for TradeProcessor
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run.sh                     # Bootstrap script
â”œâ”€â”€ Dockerfile                 # Python container for producer/consumer
â””â”€â”€ README.md
```

---

## ğŸ§  Architecture Diagram

```
+-------------------+         Kafka Topic        +-------------------------+
| Trade Producer ğŸ“¨ |  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ |  Trade Consumer ğŸ§        |
| (async Python)    |                           |  - Batch PnL Processor  |
|  - Retry + backoff|                           |  - Manual Offset Commit |
+-------------------+                           |  - DLQ fallback         |
                                                +-----------+-------------+
                                                            |
                                                            â–¼
                                                +-------------------------+
                                                |  Dead Letter Queue â˜ ï¸   |
                                                |  (Kafka DLQ Topic)      |
                                                +-------------------------+
```

---

## âœ… Features

- âœ… **Async Kafka Producer** using `aiokafka` + `asyncio`
- âœ… **Retry with exponential backoff** and jitter
- âœ… **Async Kafka Consumer** with batch processing (`getmany()`)
- âœ… **Manual offset commit** for **at-least-once delivery**
- âœ… **Dead-letter queue (DLQ)** support for poison messages
- âœ… **PnL + Validation logic** in `TradeProcessor`
- âœ… **Structured JSON logging** for observability
- âœ… **Modular code** with services/utils/models separation
- âœ… **Dockerfile** for containerization
- âœ… **Unit tests** with `pytest`

---

## ğŸ”§ Getting Started

### ğŸ³ 1. Start Kafka via Docker Compose

Use your existing Kafka setup or create a simple one:
```bash
docker-compose up -d
```

### ğŸ§ª 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### ğŸš€ 3. Run Producer

```bash
python app/producer.py
```

Simulate high-volume stream (100K trades):

```bash
HIGH_VOLUME=true python app/producer.py
```

### ğŸ“¥ 4. Run Consumer

```bash
python app/consumer.py
```

It will:
- Batch-consume from Kafka
- Validate + calculate notional
- Commit offset only after success
- Route poison messages to DLQ

---

## ğŸ§ª Running Unit Tests

```bash
pytest -q
```

---

## ğŸ³ Docker Usage

Build the image:
```bash
docker build -t trade-stream:latest .
```

Run consumer:
```bash
docker run --rm -e KAFKA_BOOTSTRAP_SERVERS=host.docker.internal:9092 -e KAFKA_TOPIC=trade-events trade-stream:latest
```

Run producer:
```bash
docker run --rm -e KAFKA_BOOTSTRAP_SERVERS=host.docker.internal:9092 -e KAFKA_TOPIC=trade-events -e HIGH_VOLUME=true trade-stream:latest python app/producer.py
```

---

## ğŸ” Delivery Guarantee

- âœ… At-least-once delivery via **manual offset commit**
- â˜ ï¸ Messages that fail processing go to **dead-letter-trade-events**
- ğŸ”„ Retries for transient Kafka errors via exponential backoff

---

## ğŸ“¸ LinkedIn Post Hashtags

When posting your project on LinkedIn, use these:

```
#Kafka #Python #EventDrivenArchitecture #FinTech #AsyncProgramming
#SoftwareEngineering #Microservices #QuantEngineering #BackOffice
#RealTimeData #aiokafka #Docker #TradeLifecycle #PnL #RiskTech
```

---

## ğŸ’¡ Sample LinkedIn Caption (edit as needed)

> Just built a Kafka + Python async trade streaming system from scratch!  
> - Modular producer/consumer  
> - Real-time validation and notional calc  
> - Retry logic, DLQ fallback, and JSON logs  
> - Ready to scale to 100K+ trades/sec ğŸš€  
> Perfect for risk teams, back office, or PnL systems.

ğŸ‘‰ Check out the repo & demo code on GitHub

---

## ğŸ§  Future Ideas

- Add Prometheus + Grafana for live metrics
- Write trades to PostgreSQL or ClickHouse
- Add gRPC server or REST API gateway
- Spark-based analytics consumer

---

Built with â¤ï¸ for production-grade event-driven systems in the finance world.